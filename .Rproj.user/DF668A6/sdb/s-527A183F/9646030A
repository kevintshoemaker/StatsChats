{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Topics\"\nauthor: \"Stats Chats\"\ndate: \"November 8, 2016\"\noutput: \n  html_document: \n    theme: flatly\n    toc: yes\n    toc_float: yes\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, cache = TRUE)\n```\n\n## Other future topics?\n\n* GIT tutorial?\n* Use of the UNR grid?\n* Ordination?\n* SQL databases?\n* Bayesian priors?\n\n# February 2017\n\n## 2/28/2017\nMostly we discussed putting together a Google spreadsheet as a one-stop shop for students to find quantitative course offerings on campus (click [here](https://docs.google.com/spreadsheets/d/1qdmvlnrLDH8bIeHXflAGDeM9MIV74G6Sm1zn2mCtqgM/edit?usp=sharing)). Next week we will discuss Justin's red-tailed hawk data set.  \n\n## 2/21/2017\nJessi Brown led a discussion/tutorial about parallelizing analyses in R using the 'parallel' package. The code will be made available on this website soon. Next week we will continue this discussion and discuss how to port this code for a Windows OS. \n\n## 2/14/2017\nFirst meeting of the semester (after a fashionable delay). Introduction and planning for the spring semester. Sounds like there is interest in workshop-type sessions focused on particular topics of interest. Among the topics discussed were parallel computing in R, SQL databases, use of \"the grid\", ordination, and GitHub. For next week: parallel computing in R, led by Stats Chats co-leader Jessi Brown. We also discussed the possibility of hosting a list of quantitative course offerings at UNR, along with a brief description of the target audience for each class.  \n\n# December 2016\n\n## 12/6/2016\n**Discussion topic:** SECR in JAGS for woodrats\n**Discussion leader:** Kevin Shoemaker and Elizabeth Hunter\nWe will discuss a model for estimating survival and movement propensity for two species of woodrats within a hybrid zone. We are using a modified CJS model in a spatially-explicit capture-recapture (secr) framework.\n\n```{r eval=FALSE}\nmodel{\n\t############\n\t#PRIORS\n\t#Survival\n\tphi0 ~ dunif(0, 1)\t\t#mean survival rate\n\tlogit.phi <- log(phi0/(1-phi0))\t#survival on logit scale\n\n\tjuvEff ~ dunif(-4,4)\t\t\t#beta term for juvenile effect on survival\n\tmaleEff ~ dunif(-4,4)\t\t\t#beta term for sex effect on survival\n\tFuEff ~ dunif(-4,4)\t\t\t#beta terms for effect of genotype on survival\n\tMaEff ~ dunif(-4,4)\n\tbcFuEff ~ dunif(-4,4)\n\tbcMaEff ~ dunif(-4,4)\n\n  \tmale.prob ~ dunif(0,1)   # probability of being male (account for unknown sex)\n\tFu.prob ~ dunif(0,1)\t # probability of being a given genotype (account for unknown genotype)\n\tMa.prob ~ dunif(0,1)\n\tbcFu.prob ~ dunif(0,1)\n\tbcMa.prob ~ dunif(0,1)\n\n\t#Some individuals are not sexed or genotyped, so need priors for those missing data\n\tfor(i in 1:n.ind){\n\t\tisMale[i] ~ dbern(male.prob)\n\t\tisFu[i] ~ dbern(Fu.prob)\n\t\tisMa[i] ~ dbern(Ma.prob)\n\t\tisbcFu[i] ~ dbern(bcFu.prob)\n\t\tisbcMa[i] ~ dbern(bcMa.prob)\n\t}\n\n  \t#Detection    \n\tlambda0.base ~ dunif(0.001, 0.5)\t# mean times resident indiv captured at its given house per independent survey event\n\t#logit.lambda0.base <- log(lambda0.base/(1-lambda0.base))\t\t\n\tsigma ~ dunif(0, 500)   # drop off in detection with distance\t\n  \tsigma2 <- pow(sigma,2)\n\n  \t#Movement among houses\n  \tp.move.male ~ dunif(0, 0.3)\n  \tp.move.female ~ dunif(0, 0.3)\n\n  \t###########\n  \t#HOUSE MODEL\n\n  \tfor (i in 1:n.ind){\n    \t\tmyHouse[i,firsts[i]] <- firstHouse[i]  #~ dcat(firstHouseProb[i,])\n    \t\tp.move[i] <- isMale[i] * p.move.male + (1-isMale[i])*p.move.female\n  \t\tfor(t in (firsts[i]+1):n.periods){\n   \t\t\ttransitionHouse[i,t] <- pow(p.move[i], period[t-1])  #Probability of transitioning to a new house\n       \t\tnewHouseCand_unif[i,t] ~ dunif(0,n.houses)    # this could be turned into a distance kernel using the ones trick?\n      \t\tnewHouseCand_cat[i,t] <- trunc(newHouseCand_unif[i,t]+1) #~ dcat(houseprob[])  #\n  \t\t\tmove[i,t] ~ dbern(transitionHouse[i,t])\t\t#latent variable: did it move to a new activity center?\n      \t\tmyHouse[i,t] <-  move[i,t]*newHouseCand_cat[i,t] + (1-move[i,t])*myHouse[i,t-1]     # which house does it live in now???  newHouse[i,t]\n  \t\t}\n  \t}\n\n\t############\n\t#OBSERVATION MODEL\n\n\tfor(i in 1:n.ind){\n\t\tfor(t in (firsts[i]+1):n.periods){\n\t\t\t#Loop only through traps near to myHouse (<200m away) that are open within a period\t\n\t\t\tfor(r in 1:nearopen.num[firstHouse[i],t]){                \n        \t\t\tlambda[i,t,nearopen[t,r,firstHouse[i]]] <- lambda0.base * exp(-1*(D2[myHouse[i,t],nearopen[t,r,firstHouse[i]]])/(2*sigma2 )) * alive[i,t]    #total exposure as function of distance\n        \t\t\tind.trap.per[i,nearopen[t,r,firstHouse[i]],t] ~ dbin(lambda[i,t,nearopen[t,r,firstHouse[i]]],eff.per[nearopen[t,r,firstHouse[i]],t])\n\t\t\t}\n\t\t}\n\t}\n\n\t############\n\t#SURVIVAL MODEL\n\n  \tfor(i in 1:n.ind){\n    \t\tfor(t in 1:(firsts[i]-1)){\n      \t\talive[i,t] ~ dbern(1)\n    \t\t}\n  \t}\n\n\tfor(i in 1:n.ind){\n\t\tfor(t in 1:(n.periods-1)){\n\t\t\tmu.phi[i,t] <- logit.phi + maleEff*isMale[i] + juvEff*isJuv[i,t] + FuEff*isFu[i] + MaEff*isMa[i] + bcFuEff*isbcFu[i] + bcMaEff*isbcMa[i]\n\t\t\t\t#EAH: changed mu.phi to be period specific instead of just [i]\n\t\t\tlogit(phi[i,t]) <- mu.phi[i,t]   # expected survival rate for this individual at this time on the basis of covariates\n\t\t}\n\t}\n\n\n\t#Latent variable: living or dead\n\tfor(i in 1:n.ind){\n\t\talive[i,firsts[i]] ~ dbern(1)\t\t#At first capture, individual is alive\n\t\tfor(t in (firsts[i]+1):n.periods){\n\t\t\ttransition[i,t] <- pow(phi[i,t-1], period[t-1])  #Probability of transitioning to this period (based on period duration)\n\t\t\tmualive[i,t] <- alive[i,(t-1)] * transition[i,t]\t#probability of each ind being alive after transitioning\n\t\t\talive[i,t] ~ dbern(mualive[i,t])\t\t#latent variable: is it still alive?\n\t\t}\n\t}\n\n} #END MODEL\n```\n\n\n# November 2016\n\n## 11/29/2016\nDiscussion topic: Mixed effects modeling. Although Pinhiero and Bates suggest that LRT is appropriate for mixed effects models, we are still unsure how to compute the degrees of freedom for the Chi-squared distribution that describes the null hypothesis. We are also wondering exactly why it is not recommended to use information criteria to compare models with different random effects structures. Paul mentioned the possibility of generating null distributions for such questions and seeing how well they match with the Chi-squared distribution recommended by pinhiero and bates.. Phillip mentioned that Ken Burnham \"solved\" this issue by using the trace of the \"G-matrix\" to approximate the number of free parameters in the model. \n\nHere are some links with good guidance!!   \n\n\n[Link1](http://stats.stackexchange.com/questions/111905/what-is-the-upside-of-treating-a-factor-as-random-in-a-mixed-model)   \n[Link2](http://stats.stackexchange.com/questions/117497/comparing-between-random-effects-structures-in-a-linear-mixed-effects-model)   \n\n## 11/22/2016\nDiscussion leader: Jessi Brown, Chris Vennum\nWe discussed what you can do with mixed models once you have fitted them using something like \"lmer\" from the \"lme4\" package in R. In partucular, we discussed how to use R to bootstrap predictions from mixed effects models and whether confidence intervals from lmer compared favorably with those from a Bayesian modeling approach in JAGS. \n\n## 11/15/2016\nLeader: Chris Vennum\nWe continued our discussion of linear mixed models- in particular, we discussed (1) strategies for evaluating the normality of residuals if the random effect terms do most of the explaining, and (2) whether parameter estimation breaks down if some or all predictor variables are perfectly correlated with a \"group\" variable (all sampling units within a group share the same covariate value). For the latter case, using a simulation (below), Thomas Riecke demonstrated that the REML algorithm in \"lmer\" was able to estimate fixed effects terms appropriately. \n\n```{r}\n#########################################################################\n# group number, cluster size\n#########################################################################\ngroups <- 50\ncluster <- 5\n\n#########################################################################\n# generate random groups\n#########################################################################\nrandom <- seq(1,groups,1)                               # 100 random groups\ndata <- data.frame(rep(random, each = cluster))            # build a data.frame with 10 observations per cluster\n\n#########################################################################\n# generate 100 covariate values\n#########################################################################\nx <- rnorm(groups, 0, 2); hist(x)\ndata[,2] <- NA\n\n#########################################################################\n# assign covariate values to groups... all the same\n#########################################################################\nfor (i in 1:nrow(data)){\n  data[i,2] <- x[data[i,1]]\n}\n\n#########################################################################\n# generate data with noise\n#########################################################################\nalpha <- 5\nbeta <- 1\ndata[,3] <- alpha + data[,2] * beta + rnorm(groups * cluster, 0, 3) \n\npar(family = 'serif', mar = c(5.1,5.1,2.1,2.1))\nplot(data[,3] ~ data[,2], ylab = 'Response', xlab = 'Covariate', cex.lab = 2)\n\n#########################################################################\n# assign less ridiculous column names\n#########################################################################\ncolnames(data) <- c('group', 'covariate', 'response')\n\nlibrary(lme4)\nsummary(lmer(response ~ covariate + (1|group), data = data))\n\n```\n\n\n## 11/8/2016 \nSince no one came with questions, we discussed mixed models. Several 'stats chats' participants over the last year have raised the question of how to determine the 'significance' of random effects, and how to decide whether to keep or remove specific random effects from a (generalized) linear mixed model. We discussed the possibility of running bootstrap tests in R to determine whether more variance is explained by the random effect than would be predicted on the basis of random chance. Paul raised the question of whether you can test for abnormal random effects distributions- e.g., bimodal distributions. We will continue this discussion next week...\n\n## 11/1/2016\nSince no one came with questions, we discussed the quantitative curriculum at UNR for graduate students and undergraduates. We started out by wondering aloud if coding and quantitive analysis could be embedded in the curriculum so that students had more exposure to these tools in many different contexts. Some key points of our discussion:  \n\n* We need to reach out to Tom Parchman in Biology to discuss whether his existing class could serve as a basic introduction to programming for grad students.    \n* Students need more consistent exposure to coding, stats, and mathematics (e.g., calc, linear algebra)\n* Bootcamps can serve a useful purpose: e.g., programming in R and python, Calculus review, linear algebra review, probability review\n* Grad students are in need of more 700-level courses. Several grad students would like to pursue a \"crash course\" in basic mathematical concepts, potentially following Otto and Day, and potentially to be offered in Fall 2017. \n\n\n\n",
    "created" : 1478646892910.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3366979710",
    "id" : "9646030A",
    "lastKnownWriteTime" : 1488397401,
    "last_content_update" : 1488397401871,
    "path" : "E:/GIT/Stats Chats/topics.Rmd",
    "project_path" : "topics.Rmd",
    "properties" : {
        "last_setup_crc32" : "DAB206197d023e20"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}